{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2\n",
    "####  Name:Bolin Yang\n",
    "\n",
    "\n",
    "Date: 09/04/2019\n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.7.0 and Anaconda 3\n",
    "\n",
    "Libraries used: \n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.7) \n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.7)\n",
    "* nltk.collocations (for finding bigrams, included in Anaconda Python 3.7)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.7)\n",
    "* from itertools import chain\n",
    "* from nltk.probability import *\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment comprises the execution of different text pre-processing, converting the extracted data into a proper format.In this assessment, it is required to write Python code to preprocess a set of information of each unit and convert them into numerical representations (which are suitable for input into recommender-systems/ information-retrieval algorithms).After converting the pdf to text by the application called Adobe Acrobat Reader DC, I divide the entire text into sections according the unit id and store information of each unit into each text.Then, write funciton and do preprocessing and store the extracted data into specified text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import library\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from nltk.corpus import reuters\n",
    "import nltk.data\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing Text\n",
    "Write functions for some steps in pre-processing but the order of the functions is same as the process of text pre-processing.The basic steps are as follows:\n",
    "* Scan pdf to text ,read text and split into separate texts\n",
    "* Write function for:\n",
    "  * Tokenization\n",
    "  * Case normlize the first word with a capital letter in the beginning of a sentence/line.\n",
    "        * Completed with regex.\n",
    "  * Use nltk PorterStemmer to do stemming.\n",
    "  * remove the vocab less than 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"This unit provides the opportunity for high-achieving students to undertake an individual research project in Genetics with an academic supervisor. It includes a critical literature review, experimental design and data analysis. The student must maintain regular contact with supervisor(s) and subject coordinator. ['Comprehend the fundamental process and requirements of scientific research;', 'Review and critically evaluate the scientific literature within a relevant discipline;', 'Demonstrate skills in experimental design, data collection and statistical analysis;', 'Interpret experimental results, and place the results in the broader context of the research discipline;', 'Communicate scientific findings and their implications, via oral presentations and written reports.'] \", \"This subject explores key issues arising from the rapid change, development and growth in international communications. It examines the impact of globalisation and shifts in production, distribution and consumption in international communications. Students will consider power and disadvantage; cultural flows and exchange; development communication; cross-cultural communication; international advertising and public relations; diasporic cultures; and legal and ethical issues in international communications. Examples will be drawn from many different countries, including case studies of communications and media in Asia, Europe, North America and the Middle East. ['have a sound understanding of globalisation as it affects communications and media industries;', 'be able to analyse global media and the policies which affect them;', 'be aware of the potentials and problems of cross-cultural communication and of the major theories which have been developed to explain them;', 'understand changing modes of reception by global audiences, together with the implications of these processes for national and personal identity;', 'be aware of the major legal and ethical challenges thrown up by the globalisation of communications and media;', 'demonstrate an enhanced ability to conduct independent research.'] \", \"This is a one week unit conducted on-campus. This program includes clinical observation in Melbourne metropolitan wound clinics and a series of presentations and workshops on specific practical wound skills including vascular assessment, measurement of ankle-brachial pressure index (ABPI) using Doppler, debridement of wounds, compression bandaging, offloading pressure and dressing product selection. ['Critically appraise the role of the multidisciplinary team within the wound care setting.', 'Integrate advanced knowledge, problem solving and current best evidence in formulating wound management plans.', 'Develop and implement a multi-disciplinary approach in the management of a patient with a wound.', 'Reflect critically on wound care management practices.', 'Communicate effectively with peers and patients on wound care related issues.']\\x0c\"]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "###use Adobe Acrobat Reader DC to transmit pdf to text\n",
    "##Read text and split it by unit ,each unit and its information are stored into a text\n",
    "rawtext= open(\"Data_file/units_pdf_totext.txt\",'r')\n",
    "line = rawtext.read()\n",
    "lines=line.replace(\"Title Synopsis Outcomes\",'') #delete the title we don't need \n",
    "name=re.findall(\"\\n[A-Z]{3}[0-9]{4}\\s\",lines)    #extract the unitid and store them into a list \n",
    "##delete the \"\\n\" in each unit id and store into a new list\n",
    "names=[]\n",
    "for item in name:\n",
    "    nn=item.replace(\"\\n\",'')\n",
    "    names.append(nn)\n",
    "\n",
    "#divide the information of each unit and delete unitid\n",
    "texlist=re.split(r\"\\n[A-Z]{3}[0-9]{4}\\s\",lines)\n",
    "lists=texlist[1:]\n",
    "#print(len(lists))\n",
    "index=0\n",
    "rawlists=[]  #delete the \\n and \\\\n of the lists\n",
    "for each in lists:\n",
    "    eachs=re.sub(\"\\n\",'',each)\n",
    "    eachs = re.sub(\"\\\\n\", '',eachs)\n",
    "    rawlists.append(eachs)\n",
    "    ##store units' information in separate text file\n",
    "    #filename=\"unitdata/\"+ str(index) +\".txt\"\n",
    "    #file= open(filename,\"w\")\n",
    "    #file.write(each)\n",
    "    #ret+=eachs\n",
    "    #index+=1\n",
    "\n",
    "#print(len(rawlist))\n",
    "print(rawlists[1:4])\n",
    "print(len(rawlists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##the word tokenization\n",
    "def token(text):\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "    #tokenizer.tokenize(text)\n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Get Stopword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove the words in stopword list\n",
    "def stopword_token(text):\n",
    "    swordsfile = \"Data_file/stopwords_en.txt\"\n",
    "    with open(swordsfile) as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "        stopwords_set = set(stopwords)\n",
    "        stopped_tokens = [w for w in text if w not in stopwords_set]\n",
    "        return stopped_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Case normalization\n",
    "* token be normalized to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_normalize(text):\n",
    "    casepattern=re.compile(r\"(^[A-Z]\\w*|[^\\w\\&,\\)\\(;-]*\\s[A-Z][a-z]+|\\n\\['[A-Z][a-z]+|,\\s'[A-Z][a-z]+)\")\n",
    "    return casepattern.sub(lambda x: x.group(0).lower(), text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Stemming\n",
    "* use Porter stemmer\n",
    "* remove the vocab less then 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proce_stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    # [w, stemmer.stem(w) for w in text]\n",
    "    stemed_tokens = list(map(lambda w: stemmer.stem(w) if not w[0].isupper() else w, text))\n",
    "\n",
    "    return stemed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_word(text):\n",
    "    fintokens = list(filter(lambda w: len(w) >= 3, text))\n",
    "    return fintokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PreProcessing\n",
    "The possible steps of text pre-processing are not the same as the steps of function as above.When do preprocessing, the steps as follow:\n",
    "* case normalization\n",
    "* word tokenization\n",
    "* remove vocab in stopword list\n",
    "* generate bigrams and retokenize\n",
    "* stem tokens use Porter stemmer\n",
    "* remove the vocab which length less than 3\n",
    "* remove the rare tokens less 5% from vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Process the first three steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ATS2060 ': 2, 'MKF5403 ': 2, 'BRM5013 ': 2, 'ATS2094 ': 2, 'OHS1000 ': 2, 'GEN5010 ': 1, 'GEN3990 ': 1, 'APG5400 ': 1, 'PGW5205 ': 1, 'LAW4122 ': 1, 'NUR5022 ': 1, 'ATS2305 ': 1, 'ATS3012 ': 1, 'BFF2401 ': 1, 'LAW4158 ': 1, 'MGS3991 ': 1, 'MKF5955 ': 1, 'BTM5909 ': 1, 'CHE2166 ': 1, 'ATS3820 ': 1, 'APG5190 ': 1, 'CIV4288 ': 1, 'ATS3145 ': 1, 'VPR1012 ': 1, 'BMA1011 ': 1, 'LAW4119 ': 1, 'ATS2169 ': 1, 'EAE2011 ': 1, 'FIT1006 ': 1, 'RSE3020 ': 1, 'ATS3908 ': 1, 'MMD4001 ': 1, 'ATS3311 ': 1, 'MAE2403 ': 1, 'FIT5137 ': 1, 'MGF5940 ': 1, 'ATS1091 ': 1, 'ASP5000 ': 1, 'ATS3189 ': 1, 'ACC3600 ': 1, 'MEC4402 ': 1, 'BMS5006 ': 1, 'AHT2740 ': 1, 'MTE4598 ': 1, 'CIV2242 ': 1, 'ATS2910 ': 1, 'RAD2061 ': 1, 'MKF5917 ': 1, 'ASP3231 ': 1, 'MKF3621 ': 1, 'GEN2041 ': 1, 'CIV6883 ': 1, 'CPS5001 ': 1, 'PGW5203 ': 1, 'MBA5213 ': 1, 'PAR3031 ': 1, 'SWM5112 ': 1, 'FIT5145 ': 1, 'PSY5261 ': 1, 'AHT4111 ': 1, 'MPH5286 ': 1, 'CIV6888 ': 1, 'ASP4002 ': 1, 'ATS3867 ': 1, 'CHE3163 ': 1, 'ADA5011 ': 1, 'SWM5241 ': 1, 'FOR5014 ': 1, 'BMH4120 ': 1, 'MKF2521 ': 1, 'ATS1211 ': 1, 'ATS3818 ': 1, 'FIT5194 ': 1, 'MDC4500 ': 1, 'MEC4407 ': 1, 'FIT3155 ': 1, 'AMU2685 ': 1, 'VCO3305 ': 1, 'MGF5985 ': 1, 'MGM5638 ': 1, 'FIT9135 ': 1, 'ATS4328 ': 1, 'NEH4012 ': 1, 'ACF5080 ': 1, 'APG5018 ': 1, 'CMH5001 ': 1, 'MEC4446 ': 1, 'CIV2225 ': 1, 'ADA2011 ': 1, 'APG5885 ': 1, 'MGF5702 ': 1, 'FIT5170 ': 1, 'FIT5011 ': 1, 'BEX3410 ': 1, 'FIT3163 ': 1, 'PAR5320 ': 1, 'MAT2742 ': 1, 'ATS3626 ': 1, 'ATS3309 ': 1, 'MKB3802 ': 1, 'ATS2490 ': 1, 'ATS2913 ': 1, 'GMA2100 ': 1, 'MEC4425 ': 1, 'SCI2400 ': 1, 'MTE4571 ': 1, 'BFC5914 ': 1, 'EAE3132 ': 1, 'IAR3401 ': 1, 'MGB2200 ': 1, 'ATS2170 ': 1, 'ATS3518 ': 1, 'PAR5430 ': 1, 'VPR3002 ': 1, 'BMS5004 ': 1, 'FIT3154 ': 1, 'EAE4010 ': 1, 'NUR4113 ': 1, 'ATS2160 ': 1, 'CHM1011 ': 1, 'AZA3688 ': 1, 'ATS3789 ': 1, 'ACC2400 ': 1, 'ATS3116 ': 1, 'FIT1044 ': 1, 'FIT5142 ': 1, 'MAE3407 ': 1, 'AMU3858 ': 1, 'IDN3002 ': 1, 'NUR2229 ': 1, 'VCM4604 ': 1, 'ATS4256 ': 1, 'ATS3135 ': 1, 'LAW4127 ': 1, 'MKB2600 ': 1, 'AHT3001 ': 1, 'BFW2341 ': 1, 'MTE5887 ': 1, 'FIT2092 ': 1, 'OCC5080 ': 1, 'AMU1304 ': 1, 'BTX3650 ': 1, 'PAR5470 ': 1, 'BTF5900 ': 1, 'MPH5313 ': 1, 'APG5717 ': 1, 'AZA3643 ': 1, 'MID2000 ': 1, 'TDN2001 ': 1, 'NUR1112 ': 1, 'HCS5100 ': 1, 'CIV6884 ': 1, 'BEX5975 ': 1, 'MKF5200 ': 1, 'IAR4119 ': 1, 'ATS1959 ': 1, 'EAE3051 ': 1, 'ATS2359 ': 1, 'BES4010 ': 1, 'AMU1328 ': 1, 'LAW4243 ': 1, 'CHE2871 ': 1, 'BMS1062 ': 1, 'AZA2789 ': 1, 'IDE4117 ': 1, 'AMU3449 ': 1, 'PAR5420 ': 1, 'FIT5125 ': 1, 'ATS1904 ': 1, 'OCC4141 ': 1, 'SWK3440 ': 1, 'ACW3491 ': 1, 'PSY5162 ': 1, 'ATS2324 ': 1, 'ATS3284 ': 1, 'EAE4064 ': 1, 'NUT3007 ': 1, 'MGF5923 ': 1, 'MCE5101 ': 1, 'AZA1264 ': 1, 'NUT5002 ': 1, 'BEX5800 ': 1, 'AZA4810 ': 1, 'MGM5966 ': 1, 'ATS3266 ': 1, 'RAD4503 ': 1, 'BTH3741 ': 1, 'IAR3118 ': 1, 'NUR5202 ': 1, 'BMA1012 ': 1, 'CIV6301 ': 1, 'FIT1049 ': 1, 'ATS1044 ': 1, 'PHH2111 ': 1, 'ASP3051 ': 1})\n",
      "200\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "## There are totally 200 units in the pdf however 5 units and their information are same \n",
    "## when we tramsform the unit and their information into a dict, so import counter package to check which units are \n",
    "## repeat and check whether their information are also repeat.\n",
    "## After that, do pre-processing \n",
    "\n",
    "from collections import Counter\n",
    "Counter(names)\n",
    "print(Counter(names))##195units are unique\n",
    "\n",
    "#lower the fist capital word in the sentence and tokenize,then remove stopwords\n",
    "token_lists=[]\n",
    "for each in rawlists:\n",
    "    nor_text=case_normalize(each)\n",
    "    token_list=token(nor_text)\n",
    "    stopp = stopword_token(token_list)\n",
    "    token_lists.append(stopp)\n",
    "#print(token_lists)\n",
    "print(len(token_lists))\n",
    "\n",
    "#store the extracted data into a dict\n",
    "data_raw={}\n",
    "if len(names)==len(token_lists):\n",
    "    for i in range(len(token_lists)):\n",
    "        data_raw[names[i]]=token_lists[i]\n",
    "\n",
    "print(len(data_raw))\n",
    "\n",
    "#count total document number\n",
    "docnum=len(data_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bigrams\n",
    "### 4.2.1 Generate bigrams\n",
    "For method bigram_finder.apply_freq_filter(min_freq), chose min_freq = 8 can get enough 200 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20284\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#Generate bigrams\n",
    "all_words= list(chain.from_iterable(data_raw.values()))\n",
    "print(len(all_words))\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "bigram_finder.apply_freq_filter(3)          \n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams\n",
    "print(len(top_200_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Retokenize the *tokenizeRawData* with *MWETokenizer*\n",
    "> Get *colloc_tokens* dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "mwetokenizer = MWETokenizer(top_200_bigrams)\n",
    "\n",
    "tott= list(mwetokenizer.tokenize(tokens) for tokens in data_raw.values())\n",
    "print(len(tott))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Remove words and stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "#pre-processing\n",
    "finall_token=[]\n",
    "\n",
    "for each in tott:\n",
    "    #stopp=stopword_token(each)\n",
    "    words=remove_word(each)\n",
    "    stemmed_token=proce_stem(words)\n",
    "    #print(stopp)\n",
    "    finall_token.append(stemmed_token)\n",
    "\n",
    "print(len(finall_token))\n",
    "\n",
    "#store extract data into a dict\n",
    "colloc_tokens={}\n",
    "for i in range(len(data_raw)):\n",
    "    vv=list(set(names))\n",
    "    colloc_tokens[vv[i]]=finall_token[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Get a vocab and Remove tokens by threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12954\n"
     ]
    }
   ],
   "source": [
    "highThresholdNumber =int( 0.95 * docnum)\n",
    "lowThresholdNumber = int(0.05 * docnum)\n",
    "\n",
    "#print(highThresholdNumber)\n",
    "#print(lowThresholdNumber)\n",
    "\n",
    "vocab = list(chain.from_iterable([set(value) for value in finall_token]))\n",
    "\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remove tokens with document frequency less than 5% or more than 95% thresholad from the vocab.<br>\n",
    "> *Get **final_List** for outpuit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_token=nltk.FreqDist(vocab)\n",
    "#print(fq_token)\n",
    "\n",
    "for key in colloc_tokens.keys():\n",
    "    colloc_tokens[key] = [w for w in colloc_tokens[key] if lowThresholdNumber < fq_token[w] < highThresholdNumber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "newVocab = list(chain.from_iterable([set(value) for value in colloc_tokens.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "source": [
    "final_list = sorted(set(newVocab))\n",
    "print(len(final_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Output for task 2\n",
    "* vocab.txt​: It contains the ​bigrams and unigrams\n",
    "tokens in the following format, ​token_string:integer_index.​ Words in\n",
    "the vocabulary must be sorted in alphabetical order.\n",
    "* countVec.txt:​ the txt file contains all the “selected”\n",
    "units in the data-set. Each line in the txt file contains the sparse representations of one of the units in the data-set in the following format ​unit_code, token_index:count, token_index:count,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabFile = \"units_inform\" + \"_vocab.txt\"\n",
    "with open(vocabFile, 'w') as f:\n",
    "    for i in range(len(final_list)):\n",
    "        line = final_list[i] + \":\" + str(i) + \"\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVecFile = \"units_inform\" + \"_countVec.txt\"\n",
    "with open(countVecFile, 'w') as f:\n",
    "    for key in colloc_tokens:\n",
    "        lines = key\n",
    "        for item in colloc_tokens[key]:\n",
    "            lines = lines + \", \" + str(final_list.index(item))+ \":\" + str(colloc_tokens[key].count(item))\n",
    "        lines = lines + \"\\n\"\n",
    "        f.write(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
